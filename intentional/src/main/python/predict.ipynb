{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "import random\n",
    "import warnings\n",
    "from minepy import cstats\n",
    "from autots import AutoTS, load_daily\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from dotenv import dotenv_values\n",
    "import numpy as np\n",
    "import cx_Oracle\n",
    "import warnings\n",
    "from matplotlib.dates import DateFormatter\n",
    "from sklearn.tree import plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.statespace.varmax import VARMAX\n",
    "from itertools import product\n",
    "warnings.filterwarnings('ignore')  # .filterwarnings(action='once') \n",
    "plt.rcParams['font.size'] = 14\n",
    "plt.rcParams['legend.fontsize'] = 10\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "plt.rcParams[\"font.family\"] = \"serif\"\n",
    "plt.rcParams[\"font.serif\"] = [\"Times New Roman\"]\n",
    "seed=42\n",
    "random.seed(seed)\n",
    "test_size=20\n",
    "sep=\"!\"\n",
    "n_iter = 10\n",
    "cv = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = dotenv_values(\"../../../.env\")\n",
    "# out_db_params = {\n",
    "#     'db_host': config[\"ORACLE_IP\"],\n",
    "#     'db_name': config[\"ORACLE_DB\"],\n",
    "#     'db_user': config[\"ORACLE_USER\"],\n",
    "#     'db_password': config[\"ORACLE_PWD\"],\n",
    "#     'db_port': config[\"ORACLE_PORT\"]\n",
    "# }\n",
    "# print(config[\"LD_LIBRARY_PATH\"])\n",
    "# cx_Oracle.init_oracle_client(lib_dir=config[\"LD_LIBRARY_PATH\"])\n",
    "# engine = create_engine('oracle+cx_oracle://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}'.format(**out_db_params))\n",
    "# query = \"SELECT * FROM cimice_dt_time\"\n",
    "# df = pd.read_sql(query, engine)\n",
    "# df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(columns=None, filters=None, file_name=None):\n",
    "    df = pd.read_csv(file_name) # esempio external knowledge\n",
    "    if columns is not None: df = df[columns]\n",
    "    if filters is not None:\n",
    "        for column, predicates in filters.items():\n",
    "            df = df[df[column].apply(lambda x: x in predicates)].reset_index(drop=True)\n",
    "    if \"week_in_year\" in list(df.columns):\n",
    "        df[\"week_in_year\"] = pd.to_datetime(df['week_in_year'] + '-1', format='%Y-%W-%w') \n",
    "    return df\n",
    "df = get_data(columns=[\"province\", \"week_in_year\", \"adults\", \"small_instars\", \"total_captures\"], filters={'province': ['BO', 'RA']}, file_name='cimice-filled.csv') # esempio interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mic(data, casualty_var):\n",
    "    \"\"\"\n",
    "    Compute the MIC matrix for the given data\n",
    "    :param data: input data\n",
    "    :param casualty_var: casualty variables to consider\n",
    "    :return: the MIC matrix\n",
    "    \"\"\"\n",
    "    X1 = data.dropna().reset_index()[casualty_var]\n",
    "    X1.columns = casualty_var\n",
    "    X = X1\n",
    "    X = X.transpose()\n",
    "    mic_c, tic_c = cstats(X, X, alpha=9, c=5, est=\"mic_e\")\n",
    "    xs = casualty_var\n",
    "    ys = casualty_var\n",
    "    mic_c = pd.DataFrame(mic_c, index=ys, columns=xs)\n",
    "    return mic_c\n",
    "\n",
    "# compute_mic(df, [target_measure] + values)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision tree (without time handled separatedly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_model(df, target_column, model, seed=seed, test_size=test_size):\n",
    "    df_enc = df.copy(deep=True)\n",
    "    # One-hot encode object columns\n",
    "    object_columns = list(df.select_dtypes(include=['object']).columns)\n",
    "    if len(object_columns) > 0: df_enc = pd.get_dummies(df_enc, drop_first=True, dtype=float, prefix_sep=sep)\n",
    "    # Convert data columns to float\n",
    "    datetime_columns = list(df_enc.select_dtypes(include=['datetime64']).columns)\n",
    "    if len(datetime_columns) > 0: df_enc[datetime_columns] = df_enc[datetime_columns].astype('int64') / 10**9\n",
    "    # Create a separate dataframe for rows with missing values in the target column\n",
    "    missing_values_df = df_enc[df_enc[target_column].isnull()]\n",
    "    if len(missing_values_df) == 0: return df, df[target_column], None, None, None, None, None, None, None\n",
    "    df_enc = df_enc.dropna(subset=[target_column])\n",
    "    # Separate target variable (what you want to predict) from features\n",
    "    X = df_enc.drop(target_column, axis=1)\n",
    "    y = df_enc[target_column]\n",
    "    # Split the data into training and testing sets\n",
    "    # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=seed, shuffle=False)\n",
    "    X_train, y_train, X_test, y_test = X[:-test_size+1], y[:-test_size+1], X[-test_size:], y[-test_size:]\n",
    "    model.fit(X_train, y_train)\n",
    "    # Get the best parameters and the best model\n",
    "    # best_params = model.best_params_\n",
    "    # best_model = model.best_estimator_\n",
    "    # print('Best Hyperparameters:', best_params)\n",
    "    y_pred = model.predict(X_test)\n",
    "    value = r2_score(y_test, y_pred)    \n",
    "    # Fill in missing values in the original dataframe\n",
    "    missing_values_df[target_column] = model.predict(missing_values_df.drop(target_column, axis=1))\n",
    "    df.loc[df[target_column].isnull(), target_column] = missing_values_df[target_column]\n",
    "    return df, df[target_column], X_train, y_train, X_test, y_test, y_pred, missing_values_df, value\n",
    "\n",
    "\n",
    "def dtree(df, target_column, date_attr=None, seed=seed, test_size=test_size):\n",
    "    # Define the hyperparameters you want to search through\n",
    "    param_grid = {\n",
    "        'max_depth': [2, 3, 4, 5],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'random_state': [seed] \n",
    "    }\n",
    "    model = RandomizedSearchCV(DecisionTreeRegressor(random_state=seed), param_grid, n_iter=n_iter, cv=cv, scoring='r2', random_state=seed)\n",
    "    return compute_model(df, target_column, model)\n",
    "\n",
    "# X, y, X_train, y_train, X_test, y_test, y_pred, missing_values_df, value = dtree(df.copy(deep=True), target_measure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forest(df, target_column, date_attr=None, seed=seed, test_size=test_size):\n",
    "    # Define hyperparameters to tune and their possible values\n",
    "    param_grid = {\n",
    "        'n_estimators': [2, 3, 4, 5],\n",
    "        'max_depth': [2, 3, 4, 5],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'random_state': [seed] \n",
    "    }\n",
    "    model = RandomizedSearchCV(RandomForestRegressor(random_state=seed), param_grid, n_iter=n_iter, cv=cv, scoring='r2', random_state=seed)\n",
    "    return compute_model(df, target_column, model)\n",
    "\n",
    "# X, y, X_train, y_train, X_test, y_test, y_pred, missing_values_df, value = forest(df.copy(deep=True), target_measure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mypivot(df, date_attr, column, exog, target_measure, impute=False):\n",
    "    if column is not None:\n",
    "        # pivot on a single column, if you want to pivot on multiple columns merge them into a single one. Necessary for melting the dataframe later\n",
    "        df = df.pivot_table(index=date_attr, columns=[column], values=exog, dropna=False)\n",
    "        df.columns = [f'{col[0]}{sep}{col[1]}' if col[1] else col[0] for col in df.columns]\n",
    "        df = df.reset_index()\n",
    "    else:\n",
    "        df.columns = [f'{x}{sep}ALL' if x != date_attr else x for x in df.columns]\n",
    "    if impute:\n",
    "        for x in [x for x in df.columns if target_measure not in x and df[x].isnull().any()]:\n",
    "            df[x] = df[x].fillna(method='ffill').fillna(method='bfill') # df[x].fillna(df[x].median()) #  \n",
    "    return df[[x for x in df.columns if \"index\" not in x]]\n",
    "\n",
    "def melt(df, date_attr, column, target_measure):\n",
    "    if column is not None:\n",
    "        df = df[[x for x in df.columns if sep not in x or target_measure in x]]\n",
    "        df = pd.melt(df, id_vars=date_attr, value_vars=[x for x in df.columns if target_measure in x], var_name=column, value_name=target_measure)\n",
    "        df[column] = df[column].apply(lambda x: x.replace(f\"{target_measure}{sep}\", \"\"))\n",
    "    return df\n",
    "\n",
    "def plot(fig, axs, cdf, date_attr, target_measure, y, X_train, y_train, X_test, y_test, y_pred, missing_values_df, value, i=0, figtitle=''):\n",
    "    # fig.suptitle(\"\" + figtitle + \"\")\n",
    "    # fig.autofmt_xdate()\n",
    "    axs[i].plot(cdf[date_attr].loc[X_train.index], y_train, label=\"Train\", c='blue')\n",
    "    axs[i].plot(cdf[date_attr].loc[X_test.index], y_test, label=\"Test\", c='blue', ls='--')\n",
    "    if y_pred is not None:\n",
    "        axs[i].plot(cdf[date_attr].loc[X_test.index], y_pred, label=\"Pred\", c='darkorange', ls='--')\n",
    "    \n",
    "    axs[i + 1].plot(cdf[date_attr], y, c='blue')\n",
    "    axs[i + 1].scatter(cdf[date_attr].loc[missing_values_df.index], missing_values_df[target_measure], label=\"Pred\", c='darkorange')\n",
    "    axs[i + 1].set_xlim([cdf[date_attr].tail(20).min(), cdf[date_attr].tail(20).max()])\n",
    "    \n",
    "    for j in range(2):\n",
    "        title = f'{target_measure.split(sep)[1]}' + (f' (R$^2$={max(0.0, round(value, 2))})' if j == 0 else '') \n",
    "        myFmt = DateFormatter(\"%Y-%W\")\n",
    "        axs[i + j].tick_params(axis='x', rotation=45)\n",
    "        axs[i + j].xaxis.set_major_formatter(myFmt)\n",
    "        axs[i + j].set_title(title)\n",
    "        axs[i + j].set_xlabel('$\\\\sf{' + date_attr.replace(\"week_in_year\", \"week\") + '}$')\n",
    "        axs[i + j].set_ylabel('$\\\\sf{' + target_measure.split(sep)[0].replace('avg', '') + '}$')\n",
    "        axs[i + j].grid()\n",
    "        # if i == 0:\n",
    "        #     # axs[i + j].legend(ncol=3)\n",
    "        #     if j == 0:\n",
    "        #         axs[i + j].legend(bbox_to_anchor=(0, 1.18, 1, 0.2), loc=\"lower left\", mode=\"expand\", borderaxespad=0, ncol=3)\n",
    "        #     else:\n",
    "        #         axs[i + j].legend(bbox_to_anchor=(0.35, 1.18, 0.3, 0.2), loc=\"lower left\", mode=\"expand\", borderaxespad=0, ncol=3)\n",
    "    fig.tight_layout()\n",
    "\n",
    "def save(fig, figtitle, target_measure):\n",
    "    fig.tight_layout()\n",
    "    for ext in [\"svg\", \"pdf\"]: fig.savefig(f\"{figtitle}_{target_measure}.{ext}\")\n",
    "    \n",
    "\n",
    "def timeseries(df, date_attr, column, target_measure, model, figtitle=\"dt\", test_size=test_size):\n",
    "    targets = [x for x in df.columns if sep in x and target_measure in x]\n",
    "    actual_targets = [c for c in targets if df[c].isnull().any()]\n",
    "    fig, axs = plt.subplots(len(actual_targets), 2, figsize=(8, 1 + 3*len(actual_targets)), sharex=False, sharey=False)  # Create a figure and subplots\n",
    "    axs = axs.flatten()  # Flatten the axs array if it's a multi-dimensional array\n",
    "    i = 0\n",
    "    for c in actual_targets:\n",
    "        cdf = df.drop(columns=[x for x in targets if x != c], axis=1)  # drop the wrong target measures \n",
    "        cdf = df.drop(columns=[x for x in df.columns if sep in x and c.split(sep)[1] not in x], axis=1)  # drop the wrong slices \n",
    "        cdf, y, X_train, y_train, X_test, y_test, y_pred, missing_values_df, value = model(cdf, c, date_attr=date_attr, test_size=test_size)  # compute the model\n",
    "        plot(fig, axs, cdf, date_attr, c, y, X_train, y_train, X_test, y_test, y_pred, missing_values_df, value, i, figtitle)\n",
    "        i += 2\n",
    "        save(fig, figtitle, c)\n",
    "    return melt(df, date_attr, column, target_measure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def sarimax(df, target_measure, date_attr, test_size=test_size, seed=seed):\n",
    "    # Create a separate dataframe for rows with missing values in the target column\n",
    "    mydf = df\n",
    "    missing_values_df = df[df.isnull().any(axis=1)]\n",
    "    missing_indices = missing_values_df.index\n",
    "    df = df.dropna()\n",
    "    exog = [x for x in df.columns if target_measure.split(sep)[0] not in x and x != date_attr]\n",
    "    X_train, y_train, X_test, y_test = df[exog][:-test_size+1], df[target_measure][:-test_size+1], df[exog][-test_size:], df[target_measure][-test_size:]\n",
    "    param_space = {\n",
    "        'p1': [1, 2, 3],\n",
    "        'p2': [1, 2, 3],\n",
    "        'p3': [1, 2, 3],\n",
    "        'p4': [1, 2, 3],\n",
    "        'p5': [1, 2, 3],\n",
    "        'p6': [1, 2, 3],\n",
    "        'p7': [4, 7, 12],\n",
    "    }\n",
    "    best_r2, best_hp, best_y_pred = float('-inf'), {}, None\n",
    "    random.seed(seed)\n",
    "    for _ in range(min(len(list(product(*param_space.values()))), n_iter)):\n",
    "        try:\n",
    "            # Generate a random set of hyperparameters\n",
    "            c_hp = {hp: random.choice(values) for hp, values in param_space.items()}\n",
    "            # Train and evaluate the model with the current set of hyperparameters\n",
    "            order = (c_hp[\"p1\"] , c_hp[\"p2\"], c_hp[\"p3\"]) # to tune \n",
    "            seasonal_order = (c_hp[\"p4\"], c_hp[\"p5\"], c_hp[\"p6\"], c_hp[\"p7\"]) # to tune\n",
    "            model = SARIMAX(endog=y_train, exog=X_train, order=order, seasonal_order=seasonal_order)\n",
    "            results = model.fit(iterations=100, disp=False)\n",
    "            y_pred = results.get_forecast(steps=test_size, exog=X_test).predicted_mean\n",
    "            y_pred.index = y_test.index\n",
    "            c_r2 = r2_score(y_test, y_pred)\n",
    "            # Update the best hyperparameters if the current configuration is better\n",
    "            if c_r2 > best_r2:\n",
    "                best_r2 = c_r2\n",
    "                best_hp = dict(c_hp)\n",
    "                best_y_pred = y_pred\n",
    "        except Exception as e:\n",
    "            print(f\"sarimax({c_hp}) - training: {e}\")\n",
    "    \n",
    "    try:\n",
    "        order = (best_hp[\"p1\"], best_hp[\"p2\"], best_hp[\"p3\"])\n",
    "        seasonal_order = (best_hp[\"p4\"], best_hp[\"p5\"], best_hp[\"p6\"], best_hp[\"p7\"])\n",
    "        model = SARIMAX(endog=df[target_measure], exog=df[exog], order=order, seasonal_order=seasonal_order)\n",
    "        results = model.fit(iterations=100, disp=False)\n",
    "        forecast = results.get_prediction(start=missing_indices[0], end=missing_indices[-1], exog=mydf[exog].loc[missing_indices]).predicted_mean\n",
    "        forecast.index = mydf.loc[missing_indices[0]:missing_indices[-1]].index\n",
    "        missing_values_df[target_measure] = forecast\n",
    "        mydf.loc[mydf[target_measure].isnull(), target_measure] = missing_values_df[target_measure]\n",
    "    except Exception as e:\n",
    "        print(f\"sarimax({c_hp}) - training: {e}\")\n",
    "        \n",
    "    return mydf, mydf[target_measure], X_train, y_train, X_test, y_test, best_y_pred, missing_values_df, best_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def varmax(df, date_attr, target_measure, test_size=test_size):\n",
    "    # Create a separate dataframe for rows with missing values in the target column\n",
    "    exog = [x for x in df.columns if target_measure.split(sep)[0] not in x and x != date_attr]\n",
    "    endo = [x for x in df.columns if target_measure in x]\n",
    "    print(f\"Exogeneous: {exog}, Endogeneous: {endo}\")\n",
    "    mydf = df\n",
    "    all_values = df\n",
    "    missing_indices = df[df.isnull().any(axis=1)].index\n",
    "    df = df.dropna()\n",
    "    X_train, Y_train, X_test, Y_test = df[exog][:-test_size+1], df[endo][:-test_size+1], df[exog][-test_size:], df[endo][-test_size:]\n",
    "    Y_pred = None\n",
    "    param_space = {\n",
    "        'p1': [1, 2, 3],\n",
    "        'p2': [1, 2, 3]\n",
    "    }\n",
    "    best_r2, best_hp, best_Y_pred = float('-inf'), {}, None\n",
    "    random.seed(seed)\n",
    "    for _ in range(min(len(list(product(*param_space.values()))), n_iter)):\n",
    "        try:\n",
    "            # Generate a random set of hyperparameters\n",
    "            c_hp = {hp: random.choice(values) for hp, values in param_space.items()}\n",
    "            model = VARMAX(endog=Y_train, exog=X_train, order=(c_hp[\"p1\"], c_hp[\"p2\"]))\n",
    "            results = model.fit(iterations=100, disp=False)\n",
    "            fcst = results.get_forecast(steps=test_size, exog=X_test)\n",
    "            Y_pred = fcst.predicted_mean\n",
    "            Y_pred.index = Y_test.index\n",
    "            c_r2 = r2_score(Y_test, Y_pred)\n",
    "            # Update the best hyperparameters if the current configuration is better\n",
    "            if c_r2 > best_r2:\n",
    "                best_r2 = c_r2\n",
    "                best_hp = dict(c_hp)\n",
    "                best_Y_pred = Y_pred\n",
    "        except Exception as e:\n",
    "            print(f\"varmax({c_hp}) - training: {e}\")\n",
    "    try:\n",
    "        c_hp = best_hp\n",
    "        model = VARMAX(endog=df[endo], exog=df[exog], order=(best_hp[\"p1\"], best_hp[\"p2\"]))\n",
    "        results = model.fit(iterations=100, disp=False)\n",
    "        forecast = results.get_prediction(start=missing_indices[0], end=missing_indices[-1], exog=mydf[exog].loc[missing_indices]).predicted_mean\n",
    "        forecast.index = all_values.loc[missing_indices[0]:missing_indices[-1]].index\n",
    "        all_values[endo] = all_values[endo].fillna(forecast)\n",
    "    except Exception as e:\n",
    "        print(f\"varmax({c_hp}) - predicting: {e}\")\n",
    "    return mydf, mydf[endo], X_train, Y_train, X_test, Y_test, best_Y_pred, forecast.loc[missing_indices], best_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_timeseries(df, date_attr, column, target_measure, model, figtitle, test_size=test_size):\n",
    "    targets = [x for x in df.columns if sep in x and target_measure in x]\n",
    "    fig, axs = plt.subplots(len(targets), 2, figsize=(8, 1 + 3*len(targets)), sharex=False, sharey=False)  # Create a figure and subplots\n",
    "    axs = axs.flatten()  # Flatten the axs array if it's a multi-dimensional array\n",
    "    i = 0\n",
    "    df, Y, X_train, Y_train, X_test, Y_test, Y_pred, missing_values_df, value = model(df, date_attr, target_measure, test_size=test_size)\n",
    "    for c in targets:\n",
    "        plot(fig, axs, df, date_attr, c, Y[c], X_train, Y_train[c], X_test, Y_test[c], Y_pred[c], missing_values_df, value, i, figtitle)\n",
    "        i += 2\n",
    "        save(fig, figtitle, c)\n",
    "    return melt(df, date_attr, column, target_measure)\n",
    "\n",
    "\n",
    "# adf = multi_timeseries(mydf, date_attr, column, target_measure, varmax, figtitle=\"multivariateTS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(df, by, target_measure, nullify_last=None):\n",
    "    date_attr = [x for x in by if \"week\" in x or \"date\" in x or \"month\" in x or \"year\" in x]\n",
    "    if len(date_attr) == 0:\n",
    "        date_attr = None\n",
    "    else:\n",
    "        date_attr = date_attr[0] # keep only one date attribute\n",
    "    column = [x for x in by if x != date_attr]\n",
    "    if len(column) == 0:\n",
    "        column = None\n",
    "    else:\n",
    "        column = column[0]\n",
    "    values = [x for x in df.columns if x not in by] # [target_measure, date_attr]\n",
    "    print(f\"date_attr: {date_attr}, column: {column}, values: {values}\")\n",
    "    if date_attr is not None:\n",
    "        pdf = mypivot(df.copy(deep=True), date_attr, column, values, target_measure, impute=True)\n",
    "        if nullify_last is not None:\n",
    "            for x in [x for x in pdf.columns if target_measure in x]:\n",
    "                for i in range(nullify_last): pdf.loc[len(pdf) - (i + 1), x] = np.nan\n",
    "        # pdf.info()\n",
    "        test_size = round(len(pdf) * 0.2)\n",
    "        timeseries(pdf.copy(deep=True), date_attr, column, target_measure, sarimax, figtitle=\"univariateTS\", test_size=test_size)\n",
    "        timeseries(pdf.copy(deep=True), date_attr, column, target_measure, forest, figtitle=\"timeRandomForest\", test_size=test_size)\n",
    "        timeseries(pdf.copy(deep=True), date_attr, column, target_measure, dtree, figtitle=\"timeDecisionTree\", test_size=test_size)\n",
    "        if column is not None and df[column].nunique() > 1:\n",
    "            multi_timeseries(pdf, date_attr, column, target_measure, varmax, figtitle=\"multivariateTS\", test_size=test_size)\n",
    "    test_size = round(len(df) * 0.2)\n",
    "    _, _, _, _, _, _, _, _, value = dtree(df.copy(deep=True), target_measure, test_size=test_size)\n",
    "    print(f\"decisionTree. R2={value}\")\n",
    "    _, _, _, _, _, _, _, _, value = forest(df.copy(deep=True), target_measure, test_size=test_size)\n",
    "    print(f\"randomForest. R2={value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_data(columns=[\"week_in_year\", \"avgadults\", \"avgsmall_instars\", \"avgcum_degree_days\"], file_name='cimice-week.csv')\n",
    "predict(df, [\"week_in_year\"], \"avgadults\", nullify_last=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_data(columns=[\"week_in_year\", \"avgadults\", \"avgsmall_instars\"], file_name='cimice-week.csv')\n",
    "predict(df, [\"week_in_year\"], \"avgadults\", nullify_last=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_data(columns=[\"week_in_year\", \"province\", \"adults\", \"small_instars\", \"total_captures\"], filters={'province': ['BO', 'RA']}, file_name='cimice-filled.csv')\n",
    "predict(df, [\"week_in_year\", \"province\"], \"adults\", nullify_last=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_data(columns=[\"week_in_year\", \"province\", \"adults\", \"small_instars\", \"total_captures\"], filters={'province': ['BO']}, file_name='cimice-filled.csv')\n",
    "predict(df, [\"week_in_year\", \"province\"], \"adults\", nullify_last=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
